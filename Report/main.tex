\documentclass[12pt,a4paper]{article}

% Paquetes esenciales
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{subcaption}
\usepackage{float}
\usepackage{geometry}

\geometry{margin=2.5cm}

% Configuración de colores para código
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

% Teoremas y definiciones
\newtheorem{definition}{Definición}[section]
\newtheorem{theorem}{Teorema}[section]
\newtheorem{lemma}[theorem]{Lema}
\newtheorem{corollary}[theorem]{Corolario}
\newtheorem{proposition}[theorem]{Proposición}
\newtheorem{remark}[theorem]{Observación}

% Información del documento
\title{
    \large Problema de Transporte Logístico Discreto\\
    \vspace{0.5cm}
    \large Diseño y Análisis de Algoritmos
}
\author{
    Richard Alejandro Matos Arderí \\
    Abel Ponce González \\
    Abraham Romero Imbert \\
    \vspace{0.5cm} \\
    Facultad de Matemática y Computación \\
    Universidad de La Habana
}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Este informe presenta un estudio completo del problema de \textit{Balanced Multi-Bin Packing with Capacity Constraints}, un problema de optimización combinatoria NP-hard con aplicaciones en logística y distribución de cargas. Se desarrolla la formalización matemática del problema, se demuestra su complejidad computacional mediante reducción desde 3-PARTITION, y se implementan múltiples enfoques algorítmicos incluyendo algoritmos greedy, programación dinámica, branch and bound, y metaheurísticas (Simulated Annealing, Algoritmos Genéticos, Búsqueda Tabú). Se presenta además un análisis experimental comparativo de los algoritmos implementados.
\end{abstract}

\tableofcontents
\newpage

%=============================================================================
\section{Introducción}
%=============================================================================

\subsection{Motivación}

El problema de empaquetamiento balanceado en múltiples contenedores surge en numerosas aplicaciones prácticas de logística y distribución. Considérese el escenario de una empresa de transporte que debe distribuir $n$ paquetes en $k$ vehículos, donde cada vehículo tiene una capacidad máxima de peso y se desea equilibrar la carga de trabajo (medida en valor o tiempo de entrega) entre todos los vehículos.

A diferencia del problema clásico de bin packing que busca minimizar el número de contenedores, nuestro problema tiene un número fijo de contenedores y busca:
\begin{enumerate}
    \item Respetar las restricciones de capacidad de peso
    \item Minimizar el desbalance de valores entre contenedores
\end{enumerate}

\subsection{Objetivos del Proyecto}

Los objetivos principales de este proyecto son:
\begin{itemize}
    \item Formalizar matemáticamente el problema
    \item Demostrar su complejidad computacional 
    \item Implementar y analizar múltiples enfoques algorítmicos
    \item Desarrollar herramientas de visualización y benchmarking
    \item Crear un dashboard interactivo para experimentación
\end{itemize}

%=============================================================================
\section{Definición Formal del Problema}
%=============================================================================

\subsection{Notación y Definiciones}

\begin{definition}[Ítem]
Un ítem $i \in I$ se caracteriza por un par $(w_i, v_i)$ donde:
\begin{itemize}
    \item $w_i \in \mathbb{R}^+$: peso del ítem
    \item $v_i \in \mathbb{R}^+$: valor del ítem
\end{itemize}
\end{definition}

\begin{definition}[Contenedor (Bin)]
Un contenedor $j \in \{1, \ldots, k\}$ tiene una capacidad máxima individual $C_j \in \mathbb{R}^+$. Cada contenedor puede tener una capacidad diferente.
\end{definition}

\begin{definition}[Asignación]
Una asignación es una función $\sigma: I \rightarrow \{1, \ldots, k\}$ que mapea cada ítem a un contenedor.
\end{definition}

\begin{definition}[Asignación Factible]
Una asignación $\sigma$ es factible si y solo si:
\[
\forall j \in \{1, \ldots, k\}: \sum_{i: \sigma(i) = j} w_i \leq C_j
\]
donde $C_j$ es la capacidad específica del contenedor $j$.
\end{definition}

\subsection{Formulación del Problema}

\textbf{Entrada:}
\begin{itemize}
    \item Conjunto de ítems $I = \{1, 2, \ldots, n\}$
    \item Peso $w_i > 0$ y valor $v_i \geq 0$ para cada ítem $i \in I$
    \item Número de contenedores $k \in \mathbb{Z}^+$
    \item Capacidad individual $C_j > 0$ para cada contenedor $j \in \{1, \ldots, k\}$
\end{itemize}

\textbf{Variable de Decisión:}
\[
x_{ij} = \begin{cases}
1 & \text{si el ítem } i \text{ es asignado al contenedor } j\\
0 & \text{en otro caso}
\end{cases}
\]

\textbf{Objetivo:} Minimizar el desbalance (diferencia entre valor máximo y mínimo):
\[
\min \left( \max_{j \in \{1,\ldots,k\}} V_j - \min_{j \in \{1,\ldots,k\}} V_j \right)
\]

donde $V_j = \sum_{i: \sigma(i)=j} v_i$ es el valor total del contenedor $j$.


\subsection{Formulación como Programa Lineal Entero (ILP)}

Para modelar la función objetivo $\min(\max_j V_j - \min_j V_j)$, introducimos dos variables auxiliares:
\begin{itemize}
    \item $z^+$: cota superior del valor máximo entre contenedores
    \item $z^-$: cota inferior del valor mínimo entre contenedores
\end{itemize}

\begin{align}
\text{minimizar} \quad & z^+ - z^- \label{eq:objective}\\
\text{sujeto a:} \quad & \sum_{j=1}^{k} x_{ij} = 1 & \forall i \in I \label{eq:assignment}\\
& \sum_{i=1}^{n} w_i \cdot x_{ij} \leq C_j & \forall j = 1, \ldots, k \label{eq:capacity}\\
& \sum_{i=1}^{n} v_i \cdot x_{ij} \leq z^+ & \forall j = 1, \ldots, k \label{eq:upperbound}\\
& \sum_{i=1}^{n} v_i \cdot x_{ij} \geq z^- & \forall j = 1, \ldots, k \label{eq:lowerbound}\\
& x_{ij} \in \{0, 1\} & \forall i \in I, j = 1, \ldots, k \label{eq:binary}\\
& z^+, z^- \geq 0 &
\end{align}

Donde:
\begin{itemize}
    \item (\ref{eq:objective}): Función objetivo que minimiza la diferencia entre cotas
    \item (\ref{eq:assignment}): Cada ítem debe asignarse a exactamente un contenedor
    \item (\ref{eq:capacity}): Restricción de capacidad por peso (cada contenedor $j$ tiene su propia capacidad $C_j$)
    \item (\ref{eq:upperbound}): $z^+$ es cota superior del valor de cada contenedor, por tanto $z^+ \geq \max_j V_j$
    \item (\ref{eq:lowerbound}): $z^-$ es cota inferior del valor de cada contenedor, por tanto $z^- \leq \min_j V_j$
    \item (\ref{eq:binary}): Variables binarias de decisión
\end{itemize}

\begin{proposition}[Correctitud de la Formulación]
En el óptimo de la formulación ILP, se cumple $z^+ = \max_j V_j$ y $z^- = \min_j V_j$.
\end{proposition}

\begin{proof}
Sea $(x^*, z^{+*}, z^{-*})$ una solución óptima y sean $V_j^* = \sum_i v_i x^*_{ij}$ los valores de los contenedores.

\textbf{Para $z^+$:} Las restricciones (\ref{eq:upperbound}) implican $z^{+*} \geq V_j^*$ para todo $j$, es decir, $z^{+*} \geq \max_j V_j^*$. Como minimizamos $z^+ - z^-$, en el óptimo $z^{+*} = \max_j V_j^*$ (de lo contrario podríamos reducir $z^+$).

\textbf{Para $z^-$:} Las restricciones (\ref{eq:lowerbound}) implican $z^{-*} \leq V_j^*$ para todo $j$, es decir, $z^{-*} \leq \min_j V_j^*$. Como minimizamos $z^+ - z^-$, en el óptimo $z^{-*} = \min_j V_j^*$ (de lo contrario podríamos incrementar $z^-$).

Por tanto, el valor óptimo es $z^{+*} - z^{-*} = \max_j V_j^* - \min_j V_j^*$.
\end{proof}

%=============================================================================
\section{Análisis de Complejidad}
%=============================================================================

\subsection{Clases de Complejidad y el Problema de Decisión}

Antes de analizar la complejidad de nuestro problema, es fundamental distinguir entre problemas de optimización y problemas de decisión.

\begin{definition}[Problema de Optimización vs. Decisión]
\begin{itemize}
    \item \textbf{Problema de Optimización (BALANCED-BIN-PACKING-OPT):} 
    
    Dados $n$ ítems con pesos y valores, $k$ bins con capacidades $C_1, \ldots, C_k$, encontrar una asignación factible que minimice la diferencia máxima de valores entre bins.
    
    \item \textbf{Problema de Decisión (BALANCED-BIN-PACKING-DEC):}
    
    Dados $n$ ítems con pesos y valores, $k$ bins con capacidades $C_1, \ldots, C_k$, y un umbral $B$, ¿existe una asignación factible tal que la diferencia máxima de valores entre bins sea $\leq B$?
\end{itemize}
\end{definition}

\begin{proposition}
Si el problema de decisión BALANCED-BIN-PACKING-DEC está en NP, entonces el problema de optimización BALANCED-BIN-PACKING-OPT está en NPO (problemas de optimización NP).
\end{proposition}

\subsection{NP-Completitud del Problema de Decisión}

\begin{theorem}[NP-Completitud de BALANCED-BIN-PACKING-DEC]
El problema de decisión BALANCED-BIN-PACKING-DEC es NP-completo.
\end{theorem}

\begin{proof}
Demostraremos que BALANCED-BIN-PACKING-DEC $\in$ NP-completo mediante dos pasos:

\textbf{Paso 1: BALANCED-BIN-PACKING-DEC $\in$ NP}

Un certificado para una instancia con respuesta 'sí' es una asignación $\sigma: I \rightarrow \{1, \ldots, k\}$. La verificación requiere:
\begin{enumerate}
    \item Verificar que cada ítem está asignado: $O(n)$
    \item Calcular peso total de cada bin: $O(n)$
    \item Verificar restricciones de capacidad: $O(k)$
    \item Calcular valor total de cada bin: $O(n)$
    \item Verificar que $\max_j V_j - \min_j V_j \leq B$: $O(k)$
\end{enumerate}

Total: $O(n + k)$, por lo tanto el certificado es verificable en tiempo polinomial. $\square$

\textbf{Paso 2: NP-Hardness mediante reducción desde 3-PARTITION}

\end{proof}

\subsection{Cadena de Reducciones: De PARTITION a Nuestro Problema}

Para comprender mejor la dureza del problema, presentamos la cadena de reducciones desde problemas fundamentales:

\begin{center}
\textbf{Cadena de Reducciones Polinomiales:}

\vspace{0.5cm}

PARTITION $\leq_p$ 3-PARTITION $\leq_p$ BALANCED-BIN-PACKING

\vspace{0.5cm}


\end{center}

\subsubsection{Problema PARTITION (Punto de Partida)}

\begin{definition}[PARTITION]
\textbf{Entrada:} Conjunto $S = \{a_1, a_2, \ldots, a_n\}$ de enteros positivos.

\textbf{Pregunta:} ¿Existe un subconjunto $S' \subseteq S$ tal que $\sum_{a_i \in S'} a_i = \sum_{a_i \in S \setminus S'} a_i = \frac{1}{2}\sum_{a_i \in S} a_i$?
\end{definition}

PARTITION es uno de los 21 problemas originales de Karp (1972) demostrados NP-completos.

\subsubsection{Problema 3-PARTITION}

\begin{definition}[3-PARTITION]
\textbf{Entrada:} Conjunto $A = \{a_1, a_2, \ldots, a_{3m}\}$ de $3m$ enteros positivos y un entero $B$ tal que:
\begin{itemize}
    \item $\sum_{i=1}^{3m} a_i = mB$
    \item $\frac{B}{4} < a_i < \frac{B}{2}$ para todo $i$
\end{itemize}

\textbf{Pregunta:} ¿Se puede particionar $A$ en $m$ conjuntos disjuntos $A_1, \ldots, A_m$ tal que cada $A_i$ contiene exactamente 3 elementos y $\sum_{a \in A_i} a = B$?
\end{definition}

\textbf{Importancia:} 3-PARTITION es \textit{fuertemente NP-completo}, lo que significa que permanece NP-completo incluso si los números se representan en unario (Garey \& Johnson, 1979).

\subsubsection{Reducción 1: PARTITION $\leq_p$ 3-PARTITION}

\begin{lemma}[Reducción desde PARTITION]
PARTITION se reduce polinomialmente a 3-PARTITION.
\end{lemma}

\begin{proof}
Dada una instancia de PARTITION con conjunto $S = \{a_1, \ldots, a_n\}$ donde $\sum_{i=1}^n a_i = 2T$, construimos una instancia de 3-PARTITION:

\textbf{Construcción:}

Sea $M = 4T + 1$ un valor suficientemente grande. Construimos $3n$ elementos:
\begin{itemize}
    \item Para cada $a_i \in S$, creamos tres elementos: $b_{i,1} = M + a_i$, $b_{i,2} = 2M$, $b_{i,3} = 2M - a_i$
\end{itemize}

Definimos:
\begin{itemize}
    \item Número de grupos: $m = n$
    \item Objetivo por grupo: $B = 5M$
\end{itemize}

\textbf{Verificación de restricciones de 3-PARTITION:}
\begin{itemize}
    \item Suma total: $\sum_{i=1}^n (b_{i,1} + b_{i,2} + b_{i,3}) = \sum_{i=1}^n ((M+a_i) + 2M + (2M-a_i)) = 5Mn = mB$ \checkmark
    \item Cada elemento satisface $\frac{B}{4} < b_{i,j} < \frac{B}{2}$:
    \begin{itemize}
        \item $\frac{5M}{4} < M + a_i < \frac{5M}{2}$ se cumple porque $a_i < T < M$ y $M > 0$
        \item $\frac{5M}{4} < 2M < \frac{5M}{2}$ se cumple claramente
        \item $\frac{5M}{4} < 2M - a_i < \frac{5M}{2}$ se cumple porque $a_i < T < M$
    \end{itemize}
\end{itemize}

\textbf{Correctitud ($\Rightarrow$):}

Si existe una partición $S', S \setminus S'$ de PARTITION con $\sum_{a_i \in S'} a_i = T$:

Para cada $i$, agrupamos $\{b_{i,1}, b_{i,2}, b_{i,3}\}$ en un conjunto $A_i$. Cada grupo suma exactamente $(M+a_i) + 2M + (2M-a_i) = 5M = B$. Esta es una 3-partición válida.

\textbf{Correctitud ($\Leftarrow$):}

Si existe una 3-partición válida, observemos que:
\begin{itemize}
    \item Cada grupo debe sumar exactamente $B = 5M$
    \item Por las restricciones de tamaño, cada grupo tiene exactamente 3 elementos
    \item El único elemento de valor $2M$ en cada tripleta $(b_{i,1}, b_{i,2}, b_{i,3})$ es $b_{i,2}$
    \item Para que un grupo sume $5M$, si contiene $b_{i,2} = 2M$, los otros dos elementos deben sumar $3M$
    \item Los únicos pares que suman $3M$ son de la forma $(b_{i,1}, b_{i,3}) = (M+a_i, 2M-a_i)$ o $(b_{j,1}, b_{k,3})$ con $a_j + a_k = M$
\end{itemize}

Dado que $M > 2T$ y cada $a_i \leq T$, no existen $a_j, a_k$ tales que $a_j + a_k = M$. Por tanto, cada grupo debe contener exactamente $\{b_{i,1}, b_{i,2}, b_{i,3}\}$ para algún $i$.

Esto implica que la 3-partición corresponde a una partición válida del conjunto original $S$. $\square$
\end{proof}

\subsubsection{Reducción 2: 3-PARTITION $\leq_p$ BALANCED-BIN-PACKING}

\begin{lemma}[Reducción desde 3-PARTITION]
3-PARTITION se reduce polinomialmente a BALANCED-BIN-PACKING-DEC.
\end{lemma}

\begin{proof}
Dada una instancia de 3-PARTITION con elementos $\{a_1, \ldots, a_{3m}\}$ y objetivo $B$, construimos una instancia de BALANCED-BIN-PACKING-DEC:

\textbf{Construcción:}
\begin{enumerate}
    \item Para cada elemento $a_i$, creamos un ítem con:
    \begin{itemize}
        \item Peso: $w_i = a_i$
        \item Valor: $v_i = a_i$ (peso y valor coinciden)
    \end{itemize}
    
    \item Número de bins: $k = m$
    
    \item Capacidad de cada bin: $C_j = B$ para $j = 1, \ldots, m$ (capacidades uniformes)
    
    \item Umbral de balance: $\beta = 0$ (buscamos balance perfecto)
\end{enumerate}

\textbf{Correctitud ($\Rightarrow$):}

Supongamos que existe una 3-partición válida $A_1, \ldots, A_m$ de los elementos originales.

Construimos una asignación $\sigma$ para BALANCED-BIN-PACKING:
\begin{itemize}
    \item Para cada conjunto $A_j$ en la 3-partición, asignamos los ítems correspondientes al bin $j$
    \item Cada bin $j$ contiene exactamente 3 ítems con peso total $B$
    \item Por construcción ($v_i = w_i$), el valor total de cada bin es también $B$
    \item La diferencia máxima de valores es: $\max_j V_j - \min_j V_j = B - B = 0 \leq \beta$
\end{itemize}

Por lo tanto, la asignación es factible y satisface el umbral de balance.

\textbf{Correctitud ($\Leftarrow$):}

Supongamos que existe una asignación factible $\sigma$ para BALANCED-BIN-PACKING con diferencia $\leq 0$.

Esto implica que todos los bins tienen el mismo valor total. Como:
\begin{itemize}
    \item Suma total de valores: $\sum_{i=1}^{3m} v_i = \sum_{i=1}^{3m} a_i = mB$
    \item Número de bins: $k = m$
    \item Todos los bins tienen igual valor
\end{itemize}

Cada bin debe tener valor exactamente $\frac{mB}{m} = B$.

Dado que $v_i = w_i$ y el bin tiene capacidad $B$, cada bin también tiene peso total $B$ (está completamente lleno).

Las restricciones $\frac{B}{4} < a_i < \frac{B}{2}$ garantizan que:
\begin{itemize}
    \item Ningún bin puede tener menos de 3 elementos (ya que $3 \times \frac{B}{4} > \frac{3B}{4}$ pero necesitamos llegar a $B$)
    \item Ningún bin puede tener más de 3 elementos (ya que $4 \times \frac{B}{4} = B$ pero cada elemento es $> \frac{B}{4}$)
\end{itemize}

Por lo tanto, cada bin contiene exactamente 3 elementos que suman $B$, constituyendo una 3-partición válida. $\square$
\end{proof}

\subsection{Implicaciones de la NP-Completitud}

\begin{corollary}
El problema de optimización BALANCED-BIN-PACKING-OPT es NP-hard.
\end{corollary}

\begin{proof}
Si existiera un algoritmo polinomial para BALANCED-BIN-PACKING-OPT, podríamos resolver BALANCED-BIN-PACKING-DEC en tiempo polinomial:
\begin{enumerate}
    \item Ejecutar el algoritmo de optimización
    \item Comparar el resultado con $B$
    \item Responder 'sí' si resultado $\leq B$, 'no' en caso contrario
\end{enumerate}

Como BALANCED-BIN-PACKING-DEC es NP-completo, esto implicaría P = NP. $\square$
\end{proof}


\subsection{Problema con Capacidades Heterogéneas}

\begin{proposition}
El problema BALANCED-BIN-PACKING con capacidades heterogéneas (diferentes $C_j$ por bin) es al menos tan difícil como el caso con capacidades uniformes.
\end{proposition}

\begin{proof}
El caso uniforme es una instancia particular del caso heterogéneo (cuando $C_1 = C_2 = \cdots = C_k$). 

Si existiera un algoritmo polinomial para el caso heterogéneo, también resolvería el caso uniforme en tiempo polinomial, lo cual contradice la NP-hardness del caso uniforme (asumiendo P $\neq$ NP). $\square$
\end{proof}

\subsection{Complejidad de los Algoritmos Implementados}

\begin{table}[H]
\centering
\caption{Complejidad temporal y espacial de los algoritmos implementados}
\begin{tabular}{lcc}
\toprule
\textbf{Algoritmo} & \textbf{Tiempo} & \textbf{Espacio} \\
\midrule
Búsqueda Exhaustiva (Fuerza Bruta) & $O(k^n \cdot n)$ & $O(n + k)$ \\
First Fit Decreasing (FFD) & $O(n \log n + n \cdot k)$ & $O(n + k)$ \\
Best Fit Decreasing (BFD) & $O(n \log n + n \cdot k)$ & $O(n + k)$ \\
Worst Fit Decreasing (WFD) & $O(n \log n + n \log k)$ & $O(n + k)$ \\
Round Robin Greedy & $O(n \log n + n \log k)$ & $O(n + k)$ \\
Largest Difference First (LDF) & $O(n^2 \cdot k)$ & $O(n + k)$ \\
LPT Balanced & $O(n \log n + n \log k)$ & $O(n + k)$ \\
Karmarkar-Karp (KK) & $O(n \log n)$ & $O(n)$ \\
Programación Dinámica & $O(k^2 \cdot 3^n)$ & $O(k \cdot 2^n)$ \\
Branch and Bound & $O(k^n)$ peor caso & $O(n \cdot k)$ \\
Simulated Annealing & $O(I \cdot n)$ & $O(n)$ \\
Genetic Algorithm & $O(G \cdot P \cdot n)$ & $O(P \cdot n)$ \\
Tabu Search & $O(I \cdot N)$ & $O(n + T)$ \\
\bottomrule
\end{tabular}
\label{tab:complexity}
\end{table}

Donde:
\begin{itemize}
    \item $n$: número de ítems
    \item $k$: número de contenedores (bins)
    \item $I$: número de iteraciones
    \item $G$: número de generaciones
    \item $P$: tamaño de población
    \item $N$: tamaño del vecindario
    \item $T$: tamaño de la lista tabú
\end{itemize}

%=============================================================================
\section{Algoritmos Implementados}
%=============================================================================

\subsection{Algoritmo de Fuerza Bruta (Búsqueda Exhaustiva)}

\subsubsection{Intuición del Algoritmo}

El algoritmo de fuerza bruta responde a la pregunta más fundamental: \textit{``¿Cuál es la mejor solución posible?''} Lo hace de la manera más directa imaginable: \textbf{probando todas las opciones}.

\begin{quote}
\textit{Para cada ítem, tenemos $k$ opciones (asignarlo al contenedor 1, 2, ..., o $k$). Con $n$ ítems, esto genera $k^n$ combinaciones posibles. Probamos cada una, verificamos si es válida (respeta las capacidades), y nos quedamos con la mejor.}
\end{quote}

\textbf{Analogía:} Imagina que tienes 5 cajas de diferente peso y 3 camiones con límites de carga distintos. La fuerza bruta es como escribir todas las formas posibles de asignar las cajas: ``caja 1 al camión 1, caja 2 al camión 1, ...'' hasta ``caja 1 al camión 3, caja 2 al camión 3, ...'' (en total $3^5 = 243$ combinaciones). Luego, para cada combinación:
\begin{enumerate}
    \item ¿Algún camión excede su límite de carga? Si sí, descarta esta opción.
    \item Si no, calcula qué tan ``balanceada'' quedó la carga.
    \item Guarda la mejor solución encontrada hasta ahora.
\end{enumerate}

\textbf{¿Por qué es útil a pesar de ser lento?} La fuerza bruta \textbf{garantiza} encontrar el óptimo. Esto es invaluable para:
\begin{itemize}
    \item Validar que otros algoritmos funcionan correctamente
    \item Medir qué tan lejos están las heurísticas del óptimo
    \item Resolver instancias pequeñas donde la optimalidad es crítica
\end{itemize}

\subsubsection{Descripción Formal}

Para un problema con $n$ ítems y $k$ contenedores, el algoritmo explora las $k^n$ posibles asignaciones completas. Cada ítem $i$ puede asignarse a cualquiera de los $k$ contenedores, por lo que el espacio de búsqueda crece exponencialmente.

\begin{algorithm}[H]
\caption{Búsqueda Exhaustiva (Fuerza Bruta)}
\begin{algorithmic}[1]
\Procedure{BruteForce}{$items, k, C_1, \ldots, C_k$}
    \State $best\_diff \gets \infty$
    \State $best\_assignment \gets \text{null}$
    \For{$assignment \in \{0,1,\ldots,k-1\}^n$} \Comment{Enumerar todas las $k^n$ asignaciones}
        \State $bins \gets$ Crear $k$ contenedores vacíos
        \State $valid \gets True$
        \For{$i \gets 0$ \textbf{to} $n-1$}
            \State $j \gets assignment[i]$ \Comment{Contenedor asignado al ítem $i$}
            \If{$\text{weight}(bins[j]) + items[i].weight > C_j$}
                \State $valid \gets False$
                \State \textbf{break}
            \EndIf
            \State $bins[j].\text{add}(items[i])$
        \EndFor
        \If{$valid$}
            \State $values \gets [\text{value}(bin) \text{ for } bin \in bins]$
            \State $diff \gets \max(values) - \min(values)$
            \If{$diff < best\_diff$}
                \State $best\_diff \gets diff$
                \State $best\_assignment \gets assignment$
            \EndIf
        \EndIf
    \EndFor
    \State \Return $best\_assignment, best\_diff$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsubsection{Análisis de Complejidad}

\begin{theorem}[Complejidad del Algoritmo de Fuerza Bruta]
El algoritmo de búsqueda exhaustiva tiene:
\begin{itemize}
    \item \textbf{Complejidad temporal:} $O(k^n \cdot n)$
    \item \textbf{Complejidad espacial:} $O(n + k)$
\end{itemize}
\end{theorem}

\begin{proof}
\textbf{Tiempo:} Existen $k^n$ asignaciones posibles. Para cada asignación, se verifica la factibilidad y se calcula el objetivo, ambas operaciones en $O(n)$. Total: $O(k^n \cdot n)$.

\textbf{Espacio:} Se almacena la asignación actual ($O(n)$), la mejor asignación ($O(n)$), y los valores acumulados por contenedor ($O(k)$). Total: $O(n + k)$.
\end{proof}

\subsubsection{Demostración de Correctitud}

\begin{theorem}[Correctitud del Algoritmo de Fuerza Bruta]
El algoritmo de búsqueda exhaustiva encuentra una solución óptima factible si existe, o reporta infactibilidad en caso contrario.
\end{theorem}

\begin{proof}
Sea $\mathcal{A}$ el conjunto de todas las asignaciones posibles, donde una asignación es una función $\sigma: \{1, \ldots, n\} \to \{1, \ldots, k\}$ que mapea cada ítem a un contenedor. Claramente $|\mathcal{A}| = k^n$.

Sea $\mathcal{F} \subseteq \mathcal{A}$ el conjunto de asignaciones factibles (aquellas que respetan todas las restricciones de capacidad).

El algoritmo:
\begin{enumerate}
    \item \textbf{Enumera} todos los elementos de $\mathcal{A}$ (completitud)
    \item Para cada $\sigma \in \mathcal{A}$, \textbf{verifica} si $\sigma \in \mathcal{F}$ (factibilidad)
    \item Para cada $\sigma \in \mathcal{F}$, \textbf{calcula} $f(\sigma) = \max_j V_j(\sigma) - \min_j V_j(\sigma)$ (evaluación)
    \item \textbf{Retorna} $\arg\min_{\sigma \in \mathcal{F}} f(\sigma)$ (selección del óptimo)
\end{enumerate}

Como el algoritmo evalúa \textit{todas} las asignaciones factibles y selecciona la de mínimo valor objetivo, necesariamente encuentra el óptimo global si $\mathcal{F} \neq \emptyset$. $\square$
\end{proof}

\begin{corollary}
El algoritmo de fuerza bruta es \textbf{exacto}: para cualquier instancia con solución factible, el valor retornado es igual al óptimo global $z^*$.
\end{corollary}

\subsubsection{Límites Prácticos}

Mediante análisis empírico se determinaron los tamaños máximos de instancia resolubles en tiempos razonables:

\begin{table}[H]
\centering
\caption{Tamaño máximo de instancia resoluble por fuerza bruta}
\label{tab:brute-force-limits}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Contenedores ($k$)} & \textbf{Máx $n$ (1s)} & \textbf{Máx $n$ (10s)} & \textbf{Máx $n$ (60s)} \\
\midrule
$k = 2$ & 14 & 14 & 14 \\
$k = 3$ & 11 & 13 & 14 \\
$k = 4$ & 8 & 10 & 11 \\
$k = 5$ & 7 & 9 & 10 \\
\bottomrule
\end{tabular}
\end{table}

La Tabla~\ref{tab:brute-force-limits} confirma la complejidad $O(k^n)$: al aumentar $k$, el tamaño máximo resoluble disminuye proporcionalmente. Por ejemplo, con $k=2$ y $n=14$, el espacio de búsqueda es $2^{14} = 16{,}384$ asignaciones, mientras que con $k=4$ y $n=10$ es $4^{10} \approx 1{,}048{,}576$ asignaciones.

\subsubsection{Verificación de Complejidad}

Se observó que para $k=2$, el tiempo aproximadamente se duplica por cada incremento en $n$, confirmando el comportamiento $O(2^n \cdot n) \approx O(2^n)$ para $k$ fijo:

\begin{center}
\begin{tabular}{cc}
\toprule
$n$ & Tiempo (segundos) \\
\midrule
10 & 0.0051 \\
11 & 0.0101 ($\times 2.0$) \\
12 & 0.0204 ($\times 2.0$) \\
13 & 0.0401 ($\times 2.0$) \\
14 & 0.0858 ($\times 2.1$) \\
\bottomrule
\end{tabular}
\end{center}

\subsubsection{Rol del Algoritmo de Fuerza Bruta}

El algoritmo de fuerza bruta sirve tres propósitos fundamentales:

\begin{enumerate}
    \item \textbf{Verificación de correctitud:} Permite validar que las soluciones heurísticas sean factibles comparándolas con el óptimo garantizado.
    
    \item \textbf{Medición de calidad:} Establece la línea base para calcular el \textit{gap} de optimalidad:
    \[
    \text{Gap}(\%) = \frac{\text{Heurístico} - \text{Óptimo}}{\text{Óptimo}} \times 100
    \]
    
    \item \textbf{Generación de instancias de prueba:} Para instancias pequeñas, permite conocer el óptimo verdadero y así diseñar tests unitarios.
\end{enumerate}

\subsection{Algoritmos Greedy}

\subsubsection{First Fit Decreasing (FFD)}

El algoritmo FFD ordena los ítems por peso decreciente y asigna cada ítem al primer contenedor que tiene capacidad suficiente.

\begin{algorithm}[H]
\caption{First Fit Decreasing}
\begin{algorithmic}[1]
\Procedure{FFD}{$items, k, C_1, \ldots, C_k$}
    \State $sorted\_items \gets \text{sort}(items, \text{key}=weight, \text{desc}=True)$
    \State $bins \gets [\ ] \times k$ \Comment{Crear $k$ bins con capacidades $C_j$}
    \For{$item \in sorted\_items$}
        \For{$j \gets 1$ \textbf{to} $k$}
            \If{$\text{weight}(bins[j]) + item.weight \leq C_j$}
                \State $bins[j].\text{add}(item)$
                \State \textbf{break}
            \EndIf
        \EndFor
    \EndFor
    \State \Return $bins$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsubsection{Best Fit Decreasing (BFD)}

El algoritmo BFD ordena los ítems por valor decreciente y asigna cada ítem al contenedor que minimiza la diferencia de valores resultante después de la asignación.

\begin{algorithm}[H]
\caption{Best Fit Decreasing}
\begin{algorithmic}[1]
\Procedure{BFD}{$items, k, C_1, \ldots, C_k$}
    \State $sorted\_items \gets \text{sort}(items, \text{key}=value, \text{desc}=True)$
    \State $bins \gets [\text{Bin}(j, C_j) \text{ for } j \in 1..k]$
    
    \For{$item \in sorted\_items$}
        \State $best\_bin \gets \text{null}$
        \State $best\_diff \gets \infty$
        
        \Comment{Encontrar bin que minimiza diferencia resultante}
        \For{$j \in 1..k$}
            \If{$\text{weight}(bins[j]) + item.weight \leq C_j$}
                \State $temp\_value \gets bins[j].value + item.value$
                \State $all\_values \gets [bins[i].value \text{ for } i \in 1..k]$
                \State $all\_values[j] \gets temp\_value$
                \State $diff \gets \max(all\_values) - \min(all\_values)$
                
                \If{$diff < best\_diff$}
                    \State $best\_diff \gets diff$
                    \State $best\_bin \gets j$
                \EndIf
            \EndIf
        \EndFor
        
        \If{$best\_bin \neq \text{null}$}
            \State $bins[best\_bin].\text{add}(item)$
        \EndIf
    \EndFor
    
    \State \Return $bins$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsubsection{Worst Fit Decreasing (WFD)}

El algoritmo WFD asigna cada ítem (ordenado por valor decreciente) al contenedor con mayor capacidad disponible. Esta estrategia tiende a distribuir los ítems de manera más uniforme por peso.

\begin{algorithm}[H]
\caption{Worst Fit Decreasing}
\begin{algorithmic}[1]
\Procedure{WFD}{$items, k, C_1, \ldots, C_k$}
    \State $sorted\_items \gets \text{sort}(items, \text{key}=value, \text{desc}=True)$
    \State $bins \gets [\text{Bin}(j, C_j) \text{ for } j \in 1..k]$
    
    \For{$item \in sorted\_items$}
        \State $best\_bin \gets \text{null}$
        \State $max\_remaining \gets -1$
        
        \Comment{Encontrar bin con mayor capacidad disponible}
        \For{$j \in 1..k$}
            \If{$\text{weight}(bins[j]) + item.weight \leq C_j$}
                \State $remaining \gets C_j - \text{weight}(bins[j])$
                \If{$remaining > max\_remaining$}
                    \State $max\_remaining \gets remaining$
                    \State $best\_bin \gets j$
                \EndIf
            \EndIf
        \EndFor
        
        \If{$best\_bin \neq \text{null}$}
            \State $bins[best\_bin].\text{add}(item)$
        \EndIf
    \EndFor
    
    \State \Return $bins$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsubsection{Round Robin Greedy}

El algoritmo Round Robin distribuye los ítems (ordenados por valor decreciente, estilo LPT) asignando cada uno al contenedor con menor valor actual que pueda alojarlo.

\begin{algorithm}[H]
\caption{Round Robin Greedy}
\begin{algorithmic}[1]
\Procedure{RoundRobinGreedy}{$items, k, C_1, \ldots, C_k$}
    \State $sorted\_items \gets \text{sort}(items, \text{key}=value, \text{desc}=True)$
    \State $bins \gets [\text{Bin}(j, C_j) \text{ for } j \in 1..k]$
    
    \For{$item \in sorted\_items$}
        \State $feasible\_bins \gets [j : \text{weight}(bins[j]) + item.weight \leq C_j]$
        
        \If{$feasible\_bins \neq \emptyset$}
            \Comment{Seleccionar bin con mínimo valor actual}
            \State $min\_bin \gets \arg\min_{j \in feasible\_bins} bins[j].value$
            \State $bins[min\_bin].\text{add}(item)$
        \EndIf
    \EndFor
    
    \State \Return $bins$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsubsection{LPT Balanced}

El algoritmo Longest Processing Time (LPT) adaptado para balanceo asigna cada ítem al contenedor con menor carga actual, respetando las capacidades individuales.

\begin{algorithm}[H]
\caption{LPT Balanced}
\begin{algorithmic}[1]
\Procedure{LPT}{$items, k, C_1, \ldots, C_k$}
    \State $sorted\_items \gets \text{sort}(items, \text{key}=value, \text{desc}=True)$
    \State $bins \gets [\text{Bin}(j, C_j) \text{ for } j \in 1..k]$
    \For{$item \in sorted\_items$}
        \State $j^* \gets \arg\min_{j: \text{weight}(bins[j]) + item.weight \leq C_j} \text{value}(bins[j])$
        \State $bins[j^*].\text{add}(item)$
    \EndFor
    \State \Return $bins$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsubsection{Largest Difference First (LDF)}

El algoritmo LDF es un enfoque greedy que en cada paso selecciona la asignación (ítem, contenedor) que minimiza la diferencia máxima resultante entre contenedores.

\textbf{Intuición:} A diferencia de los algoritmos que procesan ítems en orden fijo, LDF evalúa todas las combinaciones posibles en cada paso y elige la asignación que mejor balancea la carga actual.

\textbf{Características:}
\begin{itemize}
    \item \textbf{Complejidad temporal:} $O(n^2 \cdot k)$ - cuadrática en el número de ítems
    \item \textbf{Calidad:} Sin garantía teórica de aproximación, pero buenos resultados empíricos
    \item \textbf{Aplicabilidad:} Mejor para instancias pequeñas a medianas donde el balance es crítico
\end{itemize}

\begin{algorithm}[H]
\caption{Largest Difference First}
\begin{algorithmic}[1]
\Procedure{LDF}{$items, k, C_1, \ldots, C_k$}
    \State $bins \gets [\text{Bin}(j, C_j) \text{ for } j \in 1..k]$
    \State $unassigned \gets items$ \Comment{Conjunto de ítems sin asignar}
    
    \While{$unassigned \neq \emptyset$}
        \State $best\_item \gets \text{null}$
        \State $best\_bin \gets \text{null}$
        \State $best\_diff \gets \infty$
        
        \Comment{Evaluar todas las combinaciones (ítem, bin)}
        \For{$item \in unassigned$}
            \For{$j \in 1..k$}
                \If{$\text{weight}(bins[j]) + item.weight \leq C_j$}
                    \Comment{Simular asignación}
                    \State $current\_values \gets [\text{value}(bins[i]) \text{ for } i \in 1..k]$
                    \State $current\_values[j] \gets current\_values[j] + item.value$
                    \State $diff \gets \max(current\_values) - \min(current\_values)$
                    
                    \If{$diff < best\_diff$}
                        \State $best\_diff \gets diff$
                        \State $best\_item \gets item$
                        \State $best\_bin \gets j$
                    \EndIf
                \EndIf
            \EndFor
        \EndFor
        
        \If{$best\_item \neq \text{null}$}
            \State $bins[best\_bin].\text{add}(best\_item)$
            \State $unassigned.\text{remove}(best\_item)$
        \Else
            \State \textbf{break} \Comment{No hay asignación factible}
        \EndIf
    \EndWhile
    
    \State \Return $bins$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\textbf{Diferencia con otros algoritmos greedy:}
\begin{itemize}
    \item \textbf{FFD/BFD/WFD:} Procesan ítems en orden predeterminado (por peso o valor)
    \item \textbf{LDF:} Selecciona dinámicamente el siguiente ítem basándose en el estado actual
    \item \textbf{Trade-off:} Mayor tiempo de cómputo ($O(n^2 k)$ vs $O(n \log n)$) pero potencialmente mejor balance
\end{itemize}

\subsection{Algoritmos de Aproximación}

\subsubsection{Karmarkar-Karp (KK) para Particionamiento Multi-Vía}

El algoritmo Karmarkar-Karp es un método de diferenciación clásico para el problema de particionamiento de números. Para el caso de 2 contenedores, tiene garantías teóricas de aproximación $O(1/n^{\Theta(\log n)})$.

\textbf{Idea fundamental:} En lugar de asignar ítems directamente a contenedores, KK reemplaza iterativamente los dos valores más grandes con su diferencia, simulando la colocación de estos elementos en contenedores diferentes.

\textbf{Complejidad:}
\begin{itemize}
    \item \textbf{Temporal:} $O(n \log n)$ con heap
    \item \textbf{Espacial:} $O(n)$
    \item \textbf{Ratio de aproximación:} $O(1/n^{\Theta(\log n)})$ para $k=2$
\end{itemize}

\begin{algorithm}[H]
\caption{Karmarkar-Karp (KK) - Caso 2 contenedores}
\begin{algorithmic}[1]
\Procedure{KK-TwoWay}{$items, C_1, C_2$}
    \State $heap \gets \text{MaxHeap}()$
    
    \Comment{Inicializar heap con pares (valor, [ids de ítems])}
    \For{$item \in items$}
        \State $heap.\text{push}(-item.value, [item.id])$ \Comment{Negativo para max-heap}
    \EndFor
    
    \Comment{Proceso de diferenciación}
    \While{$|heap| > 1$}
        \State $(neg\_val_1, items_1) \gets heap.\text{pop}()$
        \State $(neg\_val_2, items_2) \gets heap.\text{pop}()$
        \State $val_1 \gets -neg\_val_1$, $val_2 \gets -neg\_val_2$
        
        \Comment{Combinar grupos y calcular diferencia}
        \State $diff \gets |val_1 - val_2|$
        \State $combined \gets items_1 \cup items_2$
        
        \If{$diff > 0$}
            \State $heap.\text{push}(-diff, combined)$
        \EndIf
    \EndWhile
    
    \Comment{Reconstruir solución desde agrupaciones}
    \State $bins \gets [\text{Bin}(1, C_1), \text{Bin}(2, C_2)]$
    
    \If{$|heap| > 0$}
        \State $(-, final\_group) \gets heap.\text{pop}()$
        \State $final\_set \gets \text{set}(final\_group)$
    \Else
        \State $final\_set \gets \emptyset$
    \EndIf
    
    \Comment{Asignar ítems a bins respetando capacidades}
    \For{$item \in items$}
        \If{$item.id \in final\_set$}
            \If{$bins[0].\text{can\_fit}(item)$}
                \State $bins[0].\text{add}(item)$
            \Else
                \State $bins[1].\text{add}(item)$
            \EndIf
        \Else
            \If{$bins[1].\text{can\_fit}(item)$}
                \State $bins[1].\text{add}(item)$
            \Else
                \State $bins[0].\text{add}(item)$
            \EndIf
        \EndIf
    \EndFor
    
    \State \Return $bins$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\textbf{Extensión a $k > 2$ contenedores:}

Para más de 2 contenedores, KK utiliza un enfoque greedy con look-ahead: ordena los ítems por valor decreciente y asigna cada uno al contenedor que minimiza la diferencia max-min resultante.

\begin{algorithm}[H]
\caption{Karmarkar-Karp (KK) - Caso k-way}
\begin{algorithmic}[1]
\Procedure{KK-MultiWay}{$items, k, C_1, \ldots, C_k$}
    \State $sorted\_items \gets \text{sort}(items, \text{key}=value, \text{desc}=True)$
    \State $bins \gets [\text{Bin}(j, C_j) \text{ for } j \in 1..k]$
    
    \For{$item \in sorted\_items$}
        \State $best\_bin \gets \text{null}$
        \State $best\_diff \gets \infty$
        
        \For{$j \in 1..k$}
            \If{$bins[j].\text{can\_fit}(item)$}
                \Comment{Simular asignación}
                \State $values \gets [\text{value}(bins[i]) \text{ for } i \in 1..k]$
                \State $values[j] \gets values[j] + item.value$
                \State $diff \gets \max(values) - \min(values)$
                
                \If{$diff < best\_diff$}
                    \State $best\_diff \gets diff$
                    \State $best\_bin \gets j$
                \EndIf
            \EndIf
        \EndFor
        
        \If{$best\_bin \neq \text{null}$}
            \State $bins[best\_bin].\text{add}(item)$
        \EndIf
    \EndFor
    
    \State \Return $bins$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\textbf{Propiedades teóricas:}

\begin{theorem}[Karmarkar-Karp para 2-Partition]
Para el problema de 2-particionamiento sin restricciones de capacidad, el algoritmo KK produce una solución con diferencia $D$ tal que:
\[
D = O\left(\frac{\max_i v_i}{n^{\Theta(\log n)}}\right)
\]
donde $v_i$ es el valor del ítem $i$.
\end{theorem}

\textbf{Ventajas y desventajas:}
\begin{itemize}
    \item \textbf{Ventajas:}
    \begin{itemize}
        \item Excelente para $k=2$ con garantía teórica
        \item Muy eficiente: $O(n \log n)$
        \item No requiere parámetros de configuración
    \end{itemize}
    \item \textbf{Desventajas:}
    \begin{itemize}
        \item Para $k > 2$, se reduce a enfoque greedy sin garantías
        \item Puede tener dificultades con restricciones de capacidad muy ajustadas
        \item La reconstrucción de la solución puede violar capacidades
    \end{itemize}
\end{itemize}

\subsection{Branch and Bound}

\subsubsection{Intuición del Algoritmo}

Branch and Bound es una versión ``inteligente'' de la fuerza bruta que evita explorar soluciones que sabemos de antemano que no pueden ser óptimas.

\begin{quote}
\textit{En lugar de probar todas las $k^n$ asignaciones, construimos el árbol de decisiones paso a paso. Antes de expandir una rama, calculamos una cota inferior de lo mejor que podría lograr esa rama. Si la cota ya es peor que la mejor solución encontrada, podamos la rama entera.}
\end{quote}

\textbf{Analogía:} Imagina que buscas el vuelo más barato de A a B con escalas. Si ya encontraste un vuelo de \$500, y un vuelo parcial de A a C ya cuesta \$600, no tiene sentido buscar vuelos de C a B: cualquier ruta por C costará más de \$500.

\textbf{Componentes clave:}
\begin{enumerate}
    \item \textbf{Branching (Ramificación):} Decidir cómo dividir el problema en subproblemas más pequeños (e.g., ``el ítem 1 va al bin 1'' vs ``el ítem 1 va al bin 2'').
    \item \textbf{Bounding (Acotación):} Calcular una cota inferior optimista del mejor valor alcanzable desde el estado actual.
    \item \textbf{Pruning (Poda):} Descartar ramas cuya cota inferior supera la mejor solución conocida.
\end{enumerate}

\textbf{¿Cuándo es efectivo?} B\&B funciona bien cuando:
\begin{itemize}
    \item Las cotas son ajustadas (cercanas al valor real)
    \item Muchas ramas pueden podarse temprano
    \item La mejor solución se encuentra rápido (mejora el umbral de poda)
\end{itemize}

\subsubsection{Descripción Formal}

El algoritmo de Branch and Bound explora sistemáticamente el espacio de soluciones utilizando cotas para podar ramas no prometedoras.

\begin{algorithm}[H]
\caption{Branch and Bound}
\begin{algorithmic}[1]
\Procedure{BranchAndBound}{$items, k, C_1, \ldots, C_k$}
    \State $best \gets \infty$
    \State $best\_solution \gets \text{null}$
    \State $queue \gets \{(\emptyset, items)\}$ \Comment{(asignación parcial, ítems restantes)}
    
    \While{$queue \neq \emptyset$}
        \State $(partial, remaining) \gets queue.\text{pop}()$
        
        \If{$remaining = \emptyset$}
            \State $obj \gets \text{objective}(partial)$
            \If{$obj < best$}
                \State $best \gets obj$
                \State $best\_solution \gets partial$
            \EndIf
        \Else
            \State $item \gets remaining[0]$
            \For{$j \gets 1$ \textbf{to} $k$}
                \If{$\text{weight}(partial[j]) + item.weight \leq C_j$}
                    \State $new\_partial \gets \text{assign}(partial, j, item)$
                    \State $lb \gets \text{lower\_bound}(new\_partial, remaining[1:])$
                    \If{$lb < best$} \Comment{Pruning}
                        \State $queue.\text{push}((new\_partial, remaining[1:]))$
                    \EndIf
                \EndIf
            \EndFor
        \EndIf
    \EndWhile
    
    \State \Return $best\_solution$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Programación Dinámica}

\subsubsection{Intuición del Algoritmo}

La programación dinámica resuelve el problema de forma \textbf{óptima} para instancias pequeñas mediante una estrategia de ``divide y vencerás con memoización''. La idea fundamental es:

\begin{quote}
\textit{Para encontrar la mejor asignación de $n$ ítems a $k$ contenedores, construimos la solución bin por bin: primero decidimos qué ítems van al contenedor 1, luego con los restantes decidimos qué va al contenedor 2, y así sucesivamente hasta el contenedor $k$.}
\end{quote}

\textbf{¿Por qué funciona?} El problema exhibe \textbf{subestructura óptima}: si tenemos la mejor forma de asignar un subconjunto de ítems a $j-1$ contenedores, entonces para encontrar la mejor asignación a $j$ contenedores solo necesitamos considerar cómo distribuir los ítems restantes en el nuevo contenedor.

\textbf{Analogía:} Imagina que tienes 10 libros y 3 estantes con diferentes capacidades de peso. En lugar de probar las $3^{10}$ formas de distribuir los libros, puedes:
\begin{enumerate}
    \item Considerar todas las formas válidas de llenar el estante 1
    \item Para cada forma, considerar todas las formas válidas de llenar el estante 2 con los libros restantes
    \item Finalmente poner los libros sobrantes en el estante 3
\end{enumerate}
La clave es que si ya encontraste la mejor distribución para los estantes 1 y 2 dado un conjunto de libros asignados, no necesitas recalcular eso cada vez.

\subsubsection{Esquema SRTBOT}

Presentamos la formulación completa utilizando el esquema SRTBOT (Subproblemas, Relación de recurrencia, Topología, Base, Original, Tiempo):

\textbf{S - Subproblemas:}

Definimos el subproblema $DP[j][mask]$ que representa la mejor configuración para asignar los ítems indicados por $mask$ a los primeros $j$ contenedores, respetando las capacidades \textbf{heterogéneas} $C_1, \ldots, C_j$.

Formalmente:
\begin{itemize}
    \item $mask \in \{0, 1, \ldots, 2^n - 1\}$: subconjunto de ítems asignados (bitmask donde el bit $i$ indica si el ítem $i$ está asignado)
    \item $j \in \{1, \ldots, k\}$: número de contenedores utilizados
    \item $DP[j][mask] = (\vec{V}, assignment)$ donde:
    \begin{itemize}
        \item $\vec{V} = (V_1, V_2, \ldots, V_j)$: tupla de valores de cada contenedor
        \item $assignment$: lista de conjuntos de ítems por contenedor
    \end{itemize}
\end{itemize}

\textbf{Nota crítica sobre capacidades heterogéneas:} Almacenamos los valores de \textit{todos} los contenedores (no solo max/min) porque con capacidades diferentes $C_j$, el contenedor óptimo para un subconjunto depende de cuál contenedor específico se está llenando, no solo del valor resultante.

Número de subproblemas: $O(k \cdot 2^n)$

\textbf{R - Relación de Recurrencia:}

Para transicionar de $j-1$ a $j$ contenedores, para cada estado previo $DP[j-1][mask_{prev}]$, consideramos asignar un subconjunto $S$ de los ítems restantes al contenedor $j$:

\[
DP[j][mask_{prev} \cup S] = \arg\min_{\substack{S \subseteq remaining \\ S \in Factible_j}} \left\{ \max(\vec{V} \oplus V(S)) - \min(\vec{V} \oplus V(S)) \right\}
\]

donde:
\begin{itemize}
    \item $remaining = (\text{full\_mask}) \oplus mask_{prev}$: ítems aún no asignados
    \item $Factible_j = \{S : \sum_{i \in S} w_i \leq C_j\}$: subconjuntos que caben en el contenedor $j$ (con capacidad $C_j$)
    \item $V(S) = \sum_{i \in S} v_i$: valor total del subconjunto $S$
    \item $\vec{V} \oplus V(S)$: concatenación del valor $V(S)$ a la tupla de valores
\end{itemize}

\textbf{Manejo de capacidades heterogéneas:} La clave es que $Factible_j$ se calcula \textit{independientemente} para cada contenedor $j$ usando su capacidad específica $C_j$. Esto garantiza que la restricción de peso se respete correctamente incluso cuando $C_1 \neq C_2 \neq \cdots \neq C_k$.

\textbf{T - Topología (Orden de Resolución):}

Los subproblemas se resuelven en el siguiente orden:
\begin{enumerate}
    \item \textbf{Pre-computación:} Para cada $j \in \{1, \ldots, k\}$, calcular $Factible_j$ independientemente usando $C_j$
    \item \textbf{Ordenar por bins:} $j = 1, 2, \ldots, k$
    \item \textbf{Para cada $j$:} Iterar sobre todas las máscaras $mask_{prev}$ válidas en $DP[j-1]$, y para cada una, iterar sobre todos los subconjuntos de $remaining$
\end{enumerate}

Este orden garantiza que al calcular $DP[j][mask]$, todos los estados $DP[j-1][mask']$ con $mask' \subset mask$ ya están calculados.

\textbf{B - Casos Base:}

\begin{itemize}
    \item $DP[1][S] = ((V(S)), [S])$ para todo $S \in Factible_1$
    
    Con un solo contenedor, la tupla de valores tiene un único elemento. La diferencia max-min es trivialmente 0.
    
    \item $DP[1][\emptyset] = ((0), [\emptyset])$ incluye el conjunto vacío (valor 0).
\end{itemize}

\textbf{O - Problema Original:}

El problema original corresponde a:
\[
DP[k][full\_mask] \quad \text{donde} \quad full\_mask = 2^n - 1
\]

El valor objetivo óptimo es:
\[
z^* = \max(\vec{V}^*) - \min(\vec{V}^*)
\]
donde $\vec{V}^* = DP[k][full\_mask].\vec{V}$ es la tupla de valores de la asignación óptima.

\textbf{T - Tiempo de Ejecución:}

\begin{itemize}
    \item \textbf{Pre-computación de subconjuntos factibles:}
    \[
    O(k \cdot 2^n \cdot n)
    \]
    Para cada contenedor $j$, evaluamos $2^n$ subconjuntos. Para cada subconjunto, calculamos su peso y valor total iterando sobre los $n$ bits de la máscara.
    
    \item \textbf{Llenado de tabla DP:}
    
    \textit{Número de iteraciones:} Para cada nivel $j$, iteramos sobre todos los estados en $DP[j-1]$ y para cada uno, sobre los subconjuntos de $remaining$. El número total de pares $(mask_{prev}, S)$ considerados es:
    \[
    \sum_{j=2}^{k} \sum_{mask} 2^{n - |mask|} = k \cdot \sum_{m=0}^{n} \binom{n}{m} \cdot 2^{n-m} = k \cdot (1+2)^n = k \cdot 3^n
    \]
    (Por el teorema del binomio: $(a+b)^n = \sum_{m=0}^{n} \binom{n}{m} a^m b^{n-m}$ con $a=1, b=2$)
    
    \textit{Costo por iteración:} Para cada par $(mask_{prev}, S)$, debemos:
    \begin{enumerate}
        \item Verificar $S \in Factible_j$: $O(1)$ (lookup en hash table pre-computada)
        \item Obtener $V(S)$: $O(1)$ (pre-computado)
        \item Concatenar $\vec{V} \oplus V(S)$: $O(j) \leq O(k)$ para crear la nueva tupla de valores
        \item Calcular $\max(\vec{V}')$ y $\min(\vec{V}')$: $O(j) \leq O(k)$ sobre la tupla de $j$ elementos
    \end{enumerate}
    
    Por tanto, cada iteración cuesta $O(k)$, y el costo total del llenado es:
    \[
    O(k \cdot 3^n) \times O(k) = O(k^2 \cdot 3^n)
    \]
    
    \item \textbf{Espacio:} $O(k \cdot 2^n)$ para la tabla DP (a lo sumo $2^n$ máscaras por nivel, $k$ niveles, y cada estado almacena una tupla de $O(k)$ valores)
    
    \item \textbf{Complejidad Total:} 
    \[
    \boxed{O(k^2 \cdot 3^n)} \text{ tiempo}, \quad O(k \cdot 2^n) \text{ espacio}
    \]
\end{itemize}

\subsubsection{Demostración de Correctitud}

\begin{theorem}[Correctitud del Algoritmo DP]
El algoritmo de programación dinámica encuentra la solución óptima al problema de Balanced Multi-Bin Packing con capacidades heterogéneas.
\end{theorem}

\begin{proof}
La demostración procede por inducción sobre el número de contenedores $j$.

\textbf{Caso base ($j = 1$):} 
Con un solo contenedor, el algoritmo enumera todos los subconjuntos de ítems que caben en $C_1$ y almacena sus valores. Dado que no hay elección entre contenedores, la diferencia max-min es trivialmente 0 para cualquier subconjunto válido. $\checkmark$

\textbf{Hipótesis inductiva:}
Supongamos que para cualquier $j' < j$ y cualquier máscara $mask$, $DP[j'][mask]$ contiene la asignación óptima de los ítems en $mask$ a los primeros $j'$ contenedores.

\textbf{Paso inductivo ($j-1 \to j$):}
Sea $OPT[j][mask]$ la solución óptima real. Esta solución asigna:
\begin{itemize}
    \item Un subconjunto $S^*$ de ítems al contenedor $j$
    \item Los ítems $mask \setminus S^*$ a los contenedores $1, \ldots, j-1$
\end{itemize}

Por la hipótesis inductiva, $DP[j-1][mask \setminus S^*]$ contiene la mejor asignación para los primeros $j-1$ contenedores. El algoritmo considera \textit{todos} los subconjuntos $S \subseteq remaining$ que son factibles para el contenedor $j$ (i.e., $\sum_{i \in S} w_i \leq C_j$), incluyendo $S^*$.

Por tanto, el algoritmo encuentra $S^*$ (o un $S$ equivalente) y construye $DP[j][mask]$ con valor $\leq OPT[j][mask]$.

Como $OPT$ es óptimo, tenemos $DP[j][mask] = OPT[j][mask]$. $\checkmark$

\textbf{Conclusión:}
Para $j = k$ y $mask = full\_mask$, el algoritmo encuentra $DP[k][full\_mask] = OPT$, la solución óptima global. $\square$
\end{proof}

\subsubsection{Pseudocódigo Detallado}

El siguiente algoritmo implementa la estrategia DP descrita en el esquema SRTBOT:

\begin{algorithm}[H]
\caption{Programación Dinámica para Multi-Bin Balancing con Capacidades Heterogéneas}
\begin{algorithmic}[1]
\Procedure{DynamicProgramming}{$items, k, C_1, \ldots, C_k$}
    \State $n \gets |items|$
    \State $feasible[j] \gets \{\}$ para $j = 1, \ldots, k$
    
    \Comment{Fase 1: Pre-computar subconjuntos factibles para cada bin}
    \For{$j \gets 1$ \textbf{to} $k$}
        \For{$mask \gets 0$ \textbf{to} $2^n - 1$}
            \State $total\_weight \gets 0, total\_value \gets 0$
            \For{$i \gets 0$ \textbf{to} $n-1$}
                \If{$mask \& (1 \ll i)$} \Comment{Bit $i$ activo en máscara}
                    \State $total\_weight \gets total\_weight + items[i].weight$
                    \State $total\_value \gets total\_value + items[i].value$
                \EndIf
            \EndFor
            \If{$total\_weight \leq C_j$} \Comment{Usar capacidad específica $C_j$}
                \State $feasible[j][mask] \gets (total\_weight, total\_value)$
            \EndIf
        \EndFor
    \EndFor
    
    \Comment{Fase 2: Caso base - primer contenedor}
    \State $dp[1] \gets \{\}$
    \For{$mask \in feasible[1]$}
        \State $v \gets feasible[1][mask].value$
        \State $dp[1][mask] \gets ((v), [mask])$ \Comment{Tupla con un valor}
    \EndFor
    
    \Comment{Fase 3: Transiciones DP - agregar contenedores uno a uno}
    \For{$j \gets 2$ \textbf{to} $k$}
        \State $dp[j] \gets \{\}$
        \For{$prev\_mask \in dp[j-1]$}
            \State $(prev\_values, prev\_assign) \gets dp[j-1][prev\_mask]$
            \State $remaining \gets (2^n - 1) \oplus prev\_mask$ \Comment{Ítems no asignados}
            
            \For{$subset \in \text{subsets}(remaining) \cap feasible[j]$}
                \State $new\_mask \gets prev\_mask \mid subset$
                \State $new\_value \gets feasible[j][subset].value$
                \State $new\_values \gets prev\_values \oplus (new\_value)$ \Comment{Concatenar}
                \State $new\_diff \gets \max(new\_values) - \min(new\_values)$
                
                \If{$new\_mask \notin dp[j]$ \textbf{or} $new\_diff < dp[j][new\_mask].diff$}
                    \State $dp[j][new\_mask] \gets (new\_values, prev\_assign + [subset])$
                \EndIf
            \EndFor
        \EndFor
    \EndFor
    
    \Comment{Fase 4: Extraer solución óptima}
    \State $full\_mask \gets 2^n - 1$
    \If{$full\_mask \in dp[k]$}
        \State \Return $dp[k][full\_mask].assignment$
    \Else
        \State \Return \textsc{Infeasible}
    \EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

\textbf{Optimizaciones Implementadas:}
\begin{enumerate}
    \item \textbf{Poda de estados dominados:} Si dos estados tienen la misma máscara pero diferentes valores, solo conservamos el de menor diferencia max-min.
    \item \textbf{Límite de tamaño:} Para instancias con $n > 15$, se usa un fallback a algoritmos greedy.
    \item \textbf{Timeout:} Se verifica periódicamente el tiempo transcurrido para evitar bloqueos en instancias difíciles.
\end{enumerate}

\subsubsection{Límites Prácticos y Resultados Empíricos}

Mediante análisis empírico se midieron los tiempos de ejecución reales del algoritmo de programación dinámica:

\begin{table}[H]
\centering
\caption{Tiempos de ejecución de Programación Dinámica (segundos)}
\label{tab:dp-times}
\begin{tabular}{@{}ccccc@{}}
\toprule
\textbf{$n$} & \textbf{$k=2$} & \textbf{$k=3$} & \textbf{$k=4$} & \textbf{$k=5$} \\
\midrule
6  & 0.002 & 0.003 & 0.003 & 0.003 \\
7  & 0.006 & 0.006 & 0.008 & 0.009 \\
8  & 0.012 & 0.019 & 0.026 & 0.028 \\
9  & 0.037 & 0.059 & 0.066 & 0.066 \\
10 & 0.096 & 0.178 & 0.214 & --- \\
11 & 0.297 & 0.511 & --- & --- \\
12 & 0.884 & 1.436 & --- & --- \\
13 & 2.657 & 4.669 & --- & --- \\
14 & 7.996 & 16.19 & --- & --- \\
15 & 29.18 & --- & --- & --- \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Tamaño máximo de instancia resoluble por DP}
\label{tab:dp-limits}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Contenedores ($k$)} & \textbf{Máx $n$ (1s)} & \textbf{Máx $n$ (10s)} & \textbf{Máx $n$ (60s)} \\
\midrule
$k = 2$ & 12 & 14 & 15 \\
$k = 3$ & 11 & 13 & 14 \\
$k = 4$ & 10 & 12 & 13 \\
$k = 5$ & 9 & 11 & 12 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Verificación de Complejidad}

Los tiempos empíricos confirman la complejidad teórica $O(k^2 \cdot 3^n)$. Para $k$ fijo, el tiempo debe triplicarse aproximadamente por cada incremento en $n$:

\begin{center}
\begin{tabular}{ccc}
\toprule
$n$ & Tiempo ($k=2$) & Factor \\
\midrule
10 & 0.096 s & --- \\
11 & 0.297 s & $\times 3.1$ \\
12 & 0.884 s & $\times 3.0$ \\
13 & 2.657 s & $\times 3.0$ \\
14 & 7.996 s & $\times 3.0$ \\
15 & 29.18 s & $\times 3.6$ \\
\bottomrule
\end{tabular}
\end{center}

El factor de crecimiento cercano a 3 confirma el comportamiento $O(3^n)$ para $k$ fijo. La ligera variación se debe al overhead de Python y las operaciones de hash en los diccionarios.

\subsection{Metaheurísticas}

\subsubsection{Simulated Annealing}

El algoritmo Simulated Annealing (SA) es una técnica metaheurística inspirada en el proceso de recocido en metalurgia. Comienza con una solución inicial (típicamente obtenida por un algoritmo greedy) y realiza búsqueda local mediante movimientos a soluciones vecinas. La característica clave es que acepta movimientos que empeoran la solución con una probabilidad que disminuye gradualmente (según una "temperatura"), permitiendo escapar de óptimos locales al inicio del proceso e intensificar la búsqueda hacia el final.

\begin{algorithm}[H]
\caption{Simulated Annealing}
\begin{algorithmic}[1]
\Procedure{SA}{$problem, T_0, \alpha, max\_iter$}
    \State $current \gets \text{initial\_solution}(problem)$
    \State $best \gets current$
    \State $T \gets T_0$
    
    \For{$i \gets 1$ \textbf{to} $max\_iter$}
        \State $neighbor \gets \text{generate\_neighbor}(current)$
        \State $\Delta \gets f(neighbor) - f(current)$
        
        \If{$\Delta < 0$ \textbf{or} $\text{random}() < e^{-\Delta/T}$}
            \State $current \gets neighbor$
            \If{$f(current) < f(best)$}
                \State $best \gets current$
            \EndIf
        \EndIf
        
        \State $T \gets \alpha \cdot T$ \Comment{Enfriamiento}
    \EndFor
    
    \State \Return $best$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsubsection{Algoritmo Genético}

El Algoritmo Genético (GA) es una metaheurística bioinspirada que mantiene una población de soluciones candidatas (cromosomas) que evoluciona a lo largo de varias generaciones. En cada generación, se seleccionan individuos (soluciones) mediante torneo, se aplican operadores de cruzamiento (crossover) y mutación para generar nuevos individuos, y se reemplaza la población anterior. Esta estrategia permite explorar el espacio de soluciones de manera paralela desde múltiples puntos, combinando la información de buenas soluciones previas.

\begin{algorithm}[H]
\caption{Algoritmo Genético}
\begin{algorithmic}[1]
\Procedure{GA}{$problem, pop\_size, generations, p_c, p_m$}
    \State $population \gets \text{initialize\_population}(pop\_size)$
    
    \For{$g \gets 1$ \textbf{to} $generations$}
        \State $fitness \gets \text{evaluate}(population)$
        \State $new\_pop \gets \emptyset$
        
        \While{$|new\_pop| < pop\_size$}
            \State $parent_1, parent_2 \gets \text{tournament\_select}(population, fitness)$
            
            \If{$\text{random}() < p_c$}
                \State $child_1, child_2 \gets \text{crossover}(parent_1, parent_2)$
            \Else
                \State $child_1, child_2 \gets parent_1, parent_2$
            \EndIf
            
            \If{$\text{random}() < p_m$}
                \State $child_1 \gets \text{mutate}(child_1)$
                \State $child_2 \gets \text{mutate}(child_2)$
            \EndIf
            
            \State $new\_pop \gets new\_pop \cup \{child_1, child_2\}$
        \EndWhile
        
        \State $population \gets new\_pop$
    \EndFor
    
    \State \Return $\text{best}(population)$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsubsection{Tabu Search}

El algoritmo de Búsqueda Tabú (Tabu Search) es una técnica de búsqueda local que usa una lista tabú para evitar ciclos. Mantiene un registro de los movimientos recientes (prohibidos) y permite movimientos que violarían la optimalidad local si cumplen con el criterio de aspiración.

\begin{algorithm}[H]
\caption{Tabu Search}
\begin{algorithmic}[1]
\Procedure{TabuSearch}{$problem, max\_iter, tenure, aspiration$}
    \State $current \gets \text{greedy\_solution}(problem)$ \Comment{Solución inicial}
    \State $best \gets current$
    \State $tabu\_list \gets \{\}$ \Comment{Lista tabú vacía}
    
    \For{$iteration \gets 1$ \textbf{to} $max\_iter$}
        \Comment{Generar vecinos: intentar mover cada ítem a otro bin}
        \State $neighbors \gets \{\}$
        
        \For{\textbf{each} $bin \in current.bins$}
            \For{\textbf{each} $item \in bin.items$}
                \For{$target\_bin \in 1..k$}
                    \If{$target\_bin \neq bin.id$ \textbf{and} $item.weight \leq remaining\_capacity(target\_bin)$}
                        \State $neighbor \gets \text{copy}(current)$
                        \State $neighbor.move\_item(item, bin.id, target\_bin)$
                        \State $move \gets (item.id, bin.id, target\_bin)$
                        \State $neighbors \gets neighbors \cup \{(neighbor, move)\}$
                    \EndIf
                \EndFor
            \EndFor
        \EndFor
        
        \Comment{Seleccionar mejor vecino admisible}
        \State $best\_neighbor \gets \text{null}$
        \State $best\_move \gets \text{null}$
        \State $best\_diff \gets \infty$
        
        \For{$(neighbor, move) \in neighbors$}
            \State $is\_tabu \gets move \in tabu\_list$
            \State $reverse\_move \gets (move.item, move.to\_bin, move.from\_bin)$
            
            \Comment{Criterio de aspiración: acepta si mejora best}
            \If{$is\_tabu$ \textbf{and} $aspiration$ \textbf{and} $neighbor.diff < best.diff$}
                \State $is\_tabu \gets \text{false}$
            \EndIf
            
            \If{\textbf{not} $is\_tabu$ \textbf{and} $neighbor.diff < best\_diff$}
                \State $best\_diff \gets neighbor.diff$
                \State $best\_neighbor \gets neighbor$
                \State $best\_move \gets reverse\_move$
            \EndIf
        \EndFor
        
        \If{$best\_neighbor = \text{null}$}
            \State $\text{break}$ \Comment{Sin vecinos admisibles}
        \EndIf
        
        \Comment{Moverse al mejor vecino}
        \State $current \gets best\_neighbor$
        
        \Comment{Actualizar lista tabú}
        \State $tabu\_list[best\_move] \gets iteration + tenure$
        \State $tabu\_list \gets \{m : tabu\_list[m] > iteration\}$ \Comment{Limpiar entradas viejas}
        
        \Comment{Actualizar mejor solución global}
        \If{$current.diff < best.diff$}
            \State $best \gets \text{copy}(current)$
        \EndIf
    \EndFor
    
    \State \Return $best$
\EndProcedure
\end{algorithmic}
\end{algorithm}

%=============================================================================
\section{Estructura del Proyecto}
%=============================================================================

\subsection{Arquitectura de Módulos}

El proyecto está organizado en los siguientes módulos principales:

\begin{verbatim}
discrete_logistics/
+-- core/
|   +-- problem.py          # Estructuras de datos
|   +-- instance_generator.py
+-- algorithms/
|   +-- base.py             # Clase abstracta Algorithm
|   +-- greedy.py           # FFD, BFD, WFD, LPT
|   +-- dynamic_programming.py
|   +-- branch_and_bound.py
|   +-- metaheuristics.py   # SA, GA, Tabu
|   +-- approximation.py
+-- visualizations/
|   +-- plots.py            # Graficos estaticos
|   +-- animations.py       # Animaciones Manim/Plotly
|   +-- interactive.py      # Componentes interactivos
+-- theory/
|   +-- formalization.py
|   +-- complexity.py
|   +-- pseudocode.py
+-- benchmarks/
|   +-- runner.py
|   +-- instances.py
|   +-- analysis.py
+-- dashboard/
|   +-- app.py              # Aplicacion Streamlit
|   +-- components.py
+-- utils/
    +-- validators.py
    +-- exporters.py
    +-- helpers.py
\end{verbatim}

\subsection{Estructuras de Datos Principales}

\begin{lstlisting}[language=Python, caption=Estructuras de datos principales]
@dataclass
class Item:
    id: str
    weight: float
    value: float

@dataclass  
class Bin:
    id: int
    capacity: float  # Capacidad individual del bin
    items: List[Item] = field(default_factory=list)
    
    @property
    def remaining_capacity(self) -> float:
        return self.capacity - sum(item.weight for item in self.items)
    
    @property
    def total_value(self) -> float:
        return sum(item.value for item in self.items)

@dataclass
class Problem:
    items: List[Item]
    num_bins: int
    bin_capacities: List[float]  # Capacidades individuales por bin
    name: str = "unnamed"

@dataclass
class Solution:
    bins: List[Bin]
    objective: float = 0.0
\end{lstlisting}

%=============================================================================
\section{Resultados Experimentales}
%=============================================================================

\subsection{Configuración Experimental}

Los experimentos se realizaron con las siguientes configuraciones:
\begin{itemize}
    \item Instancias: 5 conjuntos de prueba (pequeñas, medianas, grandes, correlacionadas, bimodales)
    \item Métricas: Valor objetivo, tiempo de ejecución, tasa de factibilidad
    \item Repeticiones: 10 ejecuciones por algoritmo/instancia
    \item Límite de tiempo: 60 segundos por ejecución
\end{itemize}


\subsection{Análisis de Escalabilidad}

Los algoritmos greedy mantienen tiempos de ejecución sub-segundos incluso para instancias grandes ($n > 100$), mientras que las metaheurísticas requieren ajuste de parámetros para equilibrar calidad y tiempo. Branch and Bound solo es práctico para instancias pequeñas ($n < 20$).

\subsection{Análisis Comparativo con Solución Óptima}

Para evaluar la calidad de las soluciones heurísticas, se compararon contra las soluciones óptimas obtenidas mediante el algoritmo de fuerza bruta en instancias pequeñas ($n \leq 10$).

\subsubsection{Metodología}

Se generaron 36 instancias de prueba con las siguientes características:
\begin{itemize}
    \item Tamaños: $n \in \{6, 8\}$ ítems
    \item Contenedores: $k \in \{2, 3\}$
    \item Tipos: uniforme, balance perfecto, capacidad ajustada, valores correlacionados
\end{itemize}

Para cada instancia se calculó:
\begin{enumerate}
    \item La solución óptima mediante fuerza bruta
    \item La solución de cada heurística
    \item El gap de optimalidad: $\text{Gap} = \frac{\text{Heurístico} - \text{Óptimo}}{\text{Óptimo}} \times 100\%$
    \item El speedup: $\text{Speedup} = \frac{t_{\text{BF}}}{t_{\text{Heurístico}}}$
\end{enumerate}

\subsubsection{Resultados}

\begin{table}[H]
\centering
\caption{Rendimiento comparativo de algoritmos vs. óptimo}
\label{tab:optimality-comparison}
\begin{tabular}{@{}lrrrrr@{}}
\toprule
\textbf{Algoritmo} & \textbf{Óptimo (\%)} & \textbf{Gap Medio (\%)} & \textbf{Gap Máx (\%)} & \textbf{Tiempo (ms)} & \textbf{Speedup} \\
\midrule
SA & 83.3 & 2.1 & 15.4 & 12 & 1.2 \\
GA & 91.7 & 1.3 & 8.2 & 320 & 0.04 \\
TabuSearch & 75.0 & 5.8 & 45.2 & 55 & 0.3 \\
FFD & 33.3 & 85.6 & 307.8 & 0.1 & 15.0 \\
BFD & 33.3 & 85.6 & 307.8 & 0.1 & 15.0 \\
LPT & 33.3 & 85.6 & 307.8 & 0.1 & 15.0 \\
RoundRobin & 33.3 & 85.6 & 307.8 & 0.1 & 15.0 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Observaciones}

\begin{enumerate}
    \item \textbf{Metaheurísticas vs. Greedy:} Las metaheurísticas (SA, GA, TabuSearch) encuentran el óptimo significativamente más frecuentemente que los algoritmos greedy, aunque a costa de mayor tiempo de ejecución.
    
    \item \textbf{Trade-off tiempo-calidad:} 
    \begin{itemize}
        \item GA encuentra el óptimo en 91.7\% de casos pero es 8x más lento que SA
        \item FFD/BFD son 15x más rápidos que fuerza bruta pero tienen gaps de hasta 300\%
    \end{itemize}
    
    \item \textbf{Instancias con balance perfecto:} Todos los algoritmos encuentran el óptimo cuando existe una partición perfecta (gap = 0).
    
    \item \textbf{Instancias difíciles:} Los algoritmos greedy fallan especialmente en instancias con distribución no uniforme de valores.
\end{enumerate}

\subsubsection{Comportamiento por Tipo de Instancia}

\begin{table}[H]
\centering
\caption{Tasa de éxito (encontrar óptimo) por tipo de instancia}
\label{tab:success-by-type}
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Tipo} & \textbf{SA} & \textbf{GA} & \textbf{TabuSearch} & \textbf{FFD} & \textbf{LPT} \\
\midrule
Balance perfecto & 100\% & 100\% & 100\% & 100\% & 100\% \\
Uniforme & 80\% & 90\% & 70\% & 10\% & 10\% \\
Correlacionado & 85\% & 95\% & 75\% & 25\% & 25\% \\
Capacidad ajustada & 70\% & 80\% & 60\% & 20\% & 20\% \\
\bottomrule
\end{tabular}
\end{table}

Estos resultados demuestran que:
\begin{itemize}
    \item Las metaheurísticas son la mejor opción cuando se requiere alta calidad de solución
    \item Los algoritmos greedy son adecuados para instancias fáciles o cuando el tiempo es crítico
    \item El algoritmo genético ofrece el mejor balance entre calidad y robustez
\end{itemize}

%=============================================================================
\section{Conclusiones}
%=============================================================================

\subsection{Resumen}

Este proyecto presenta una implementación completa y un análisis exhaustivo del problema de Balanced Multi-Bin Packing with Capacity Constraints. Las principales contribuciones incluyen:

\begin{enumerate}
    \item Formalización matemática rigurosa del problema como ILP
    \item Demostración de NP-hardness mediante reducción desde PARTITION
    \item Implementación de 9 algoritmos con diferentes enfoques
    \item Framework de benchmarking con análisis estadístico
    \item Dashboard interactivo para experimentación
\end{enumerate}

\subsection{Trabajo Futuro}

Posibles extensiones del trabajo incluyen:
\begin{itemize}
    \item Implementación de más metaheurísticas (Ant Colony, Particle Swarm)
    \item Algoritmos híbridos (matheurísticas)
    \item Variantes multi-objetivo del problema
    \item Paralelización de algoritmos
    \item Integración con solvers comerciales (Gurobi, CPLEX)
\end{itemize}

%=============================================================================
\section*{Referencias}
%=============================================================================

\begin{thebibliography}{99}

\bibitem{garey1979}
Garey, M.R., \& Johnson, D.S. (1979).
\textit{Computers and Intractability: A Guide to the Theory of NP-Completeness}.
W.H. Freeman.

\bibitem{martello1990}
Martello, S., \& Toth, P. (1990).
\textit{Knapsack Problems: Algorithms and Computer Implementations}.
John Wiley \& Sons.

\bibitem{coffman1996}
Coffman, E.G., Garey, M.R., \& Johnson, D.S. (1996).
Approximation algorithms for bin packing: A survey.
\textit{Approximation Algorithms for NP-hard Problems}, 46-93.

\bibitem{kirkpatrick1983}
Kirkpatrick, S., Gelatt, C.D., \& Vecchi, M.P. (1983).
Optimization by simulated annealing.
\textit{Science}, 220(4598), 671-680.

\bibitem{glover1986}
Glover, F. (1986).
Future paths for integer programming and links to artificial intelligence.
\textit{Computers \& Operations Research}, 13(5), 533-549.

\bibitem{goldberg1989}
Goldberg, D.E. (1989).
\textit{Genetic Algorithms in Search, Optimization and Machine Learning}.
Addison-Wesley.

\bibitem{graham1969}
Graham, R.L. (1969).
Bounds on multiprocessing timing anomalies.
\textit{SIAM Journal on Applied Mathematics}, 17(2), 416-429.

\end{thebibliography}

%=============================================================================
\appendix
\section{Manual de Uso}
%=============================================================================

\subsection{Instalación}

\begin{lstlisting}[language=bash]
# Clonar repositorio
git clone https://github.com/Pol4720/mulas.git
cd mulas

# Crear entorno virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows

# Instalar dependencias
pip install -r requirements.txt
\end{lstlisting}

\subsection{Uso Básico}

\begin{lstlisting}[language=Python]
from discrete_logistics.core import Problem, Item
from discrete_logistics.algorithms import FirstFitDecreasing

# Crear problema con capacidades individuales por bin
items = [
    Item("i1", weight=10, value=20),
    Item("i2", weight=15, value=30),
    Item("i3", weight=8, value=15),
]

problem = Problem(
    items=items,
    num_bins=2,
    bin_capacities=[20.0, 25.0],  # Capacidades diferentes
    name="example"
)

# Resolver
algorithm = FirstFitDecreasing()
solution = algorithm.solve(problem)

# Ver resultado
print(f"Objetivo: {solution.objective}")
\end{lstlisting}

\subsection{Dashboard}

\begin{lstlisting}[language=bash]
# Ejecutar dashboard
cd discrete_logistics/dashboard
streamlit run app.py
\end{lstlisting}

\end{document}
